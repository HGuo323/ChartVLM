# -*- coding: utf-8 -*-
"""basemodel_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tIhBOUa_kkqItsXAMqBkvxBEmaldxYvz
"""

!pip install datasets

import torch
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
from datasets import load_dataset
from tqdm import tqdm

model_name = "ahmed-masry/chartgemma"
processor = AutoProcessor.from_pretrained(model_name)
model = PaliGemmaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

dataset = load_dataset("ahmed-masry/ChartQA", split="val")

print(dataset.column_names)

import re
import math

def normalize_number_string(s):
    """
    Normalize numeric string: strip %, make float if possible.
    Returns a string and a float (if parseable).
    """
    s_clean = s.strip().replace(",", "")
    s_num_only = re.sub(r"[^\d.]", "", s_clean)
    try:
        return s_clean.lower(), float(s_num_only)
    except:
        return s_clean.lower(), None

def compute_metric(gt, pred, tol=1e-4):
    gt_str, gt_val = normalize_number_string(gt)
    pred_str, pred_val = normalize_number_string(pred)

    if gt_val is not None and pred_val is not None:
        if math.isclose(gt_val, pred_val, rel_tol=tol):
            return True
        # if math.isclose(gt_val, pred_val * 100, rel_tol=tol):
        #     return True
        # if math.isclose(gt_val * 100, pred_val, rel_tol=tol):
        #     return True

    # return gt_str == pred_str or gt_str in pred_str or pred_str in gt_str
    return gt_str == pred_str

import numpy as np
from PIL import Image
import io
import re


scores = []
for idx, example in enumerate(tqdm(dataset)):
    image = Image.open(io.BytesIO(example["image"])).convert("RGB")
    question = example["query"]
    answer = str(example["label"]).strip()

    encoded = processor(text=[question], images=[image], return_tensors="pt")
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)
    pixel_values = encoded["pixel_values"].to(device)

    with torch.no_grad():
        generated_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            max_new_tokens=128
        )

    full_pred = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    stripped_pred = processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)[0]

    pred = stripped_pred if stripped_pred else full_pred
    pred = pred.strip()

    is_correct = compute_metric(answer, pred)
    scores.append(1 if is_correct else 0)


    print(f"Question      : {question}")
    print(f"Prediction    : {pred}")
    print(f"Ground Truth  : {answer}")
    print(f"Match         : {'✅' if is_correct else '❌'}")

accuracy = np.mean(scores)
print(f"\n✅ val_relaxed_accuracy: {accuracy:.4f} ({sum(scores)}/{len(scores)})")

import matplotlib.pyplot as plt
from datasets import load_dataset
from PIL import Image
import io


i = 0
example = dataset[i]

image = Image.open(io.BytesIO(example["image"])).convert("RGB")

question = example["query"]
answer = str(example["label"])
print(f"Question: {question}")
print(f"Answer: {answer}")

plt.figure(figsize=(8, 6))
plt.imshow(image)
plt.axis("off")
# plt.title("ChartQA Sample", fontsize=16)
# plt.figtext(0.5, 0.01, f"Q: {question}\nA: {answer}", wrap=True, horizontalalignment='center', fontsize=12)
plt.tight_layout()
plt.show()